---
phase: 11-database-integration-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/webapp/test_db.py
autonomous: true

must_haves:
  truths:
    - "A job can be inserted via upsert_job(), read back via get_job(), and all stored fields match the input dict"
    - "A job can transition through all 11 lifecycle statuses (discovered through ghosted) via update_job_status() and each transition persists correctly"
    - "Setting status to 'applied' also sets applied_date to a non-null timestamp"
    - "update_job_notes() persists notes text retrievable via get_job()"
    - "mark_viewed() sets viewed_at on first call and is idempotent on subsequent calls"
    - "upsert_jobs() processes a batch of N jobs and returns N"
    - "Bulk status update via sequential update_job_status() calls changes exactly those N jobs and no others"
    - "remove_stale_jobs() deletes jobs from searched platforms with old timestamps but preserves unsearched platforms"
    - "backfill_score_breakdowns() re-scores jobs that have score but no breakdown, skips jobs with existing breakdown"
    - "record_run() inserts a run history entry retrievable via get_run_history()"
    - "get_stats() returns correct totals grouped by score, status, and platform"
    - "Database initialization via init_db() creates all required tables (jobs, activity_log, run_history, resume_versions, jobs_fts), indexes (idx_activity_dedup, idx_activity_created, idx_resume_versions_job), and triggers (jobs_fts_ai, jobs_fts_ad, jobs_fts_au)"
    - "PRAGMA user_version equals 6 after full migration"
    - "Calling init_db() twice is idempotent -- no errors on second call"
  artifacts:
    - path: "tests/webapp/test_db.py"
      provides: "DB-01, DB-04, DB-05: CRUD lifecycle, bulk operations, and schema initialization integration tests"
      contains: "class TestCrudLifecycle"
  key_links:
    - from: "tests/webapp/test_db.py"
      to: "webapp/db.py"
      via: "Direct import of db module functions"
      pattern: "import webapp.db as db_module"
---

<objective>
Write integration tests for DB CRUD lifecycle (DB-01), bulk status operations (DB-04), and schema initialization (DB-05).

Purpose: These tests verify the write-side and structural foundation of webapp/db.py -- that jobs can be inserted, updated through all lifecycle states, bulk-modified, and that the database schema is correctly initialized with all tables, indexes, triggers, and migrations.

Output: tests/webapp/test_db.py with test classes covering CRUD lifecycle, bulk operations, run history, stats, backfill, and schema verification.
</objective>

<execution_context>
@/Users/patrykattc/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrykattc/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-database-integration-tests/11-RESEARCH.md
@webapp/db.py
@tests/conftest.py
@tests/test_delta.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tests/webapp/test_db.py with CRUD lifecycle, bulk, stats, run history, and backfill tests (DB-01, DB-04)</name>
  <files>tests/webapp/test_db.py</files>
  <action>
Create `tests/webapp/test_db.py` with integration tests. Mark all test classes with `@pytest.mark.integration`.

**Test helpers** (module-level, reuse exact pattern from `tests/test_delta.py`):

```python
def _make_job_dict(company, title, platform="indeed", **kwargs) -> dict:
    # Same helper as test_delta.py -- builds minimal valid job dict for upsert_job()

def _compute_dedup_key(company, title) -> str:
    # Same helper as test_delta.py -- replicates dedup_key normalization
```

**class TestCrudLifecycle (DB-01):**

- `test_insert_and_read_back`: Insert via `upsert_job(_make_job_dict("Google", "Staff Engineer"))`, read via `get_job(key)`. Assert all stored fields match: title, company, platform, location, url, description, status=="discovered".
- `test_upsert_preserves_longer_description`: Insert with short description, re-upsert same dedup_key with longer description. Verify the longer description wins (the ON CONFLICT clause uses `LENGTH` comparison).
- `test_upsert_preserves_existing_score`: Insert with `score=4`, re-upsert same job with `score=None`. Verify score stays 4 (COALESCE in ON CONFLICT keeps existing).
- `test_get_job_nonexistent_returns_none`: `get_job("nonexistent::key")` returns `None`.
- `test_status_transition` (parametrize all 11 statuses: discovered, scored, saved, applied, phone_screen, technical, final_interview, offer, rejected, withdrawn, ghosted): Insert job, call `update_job_status(key, status)`, verify `get_job(key)["status"] == status`. Use explicit `ids=lambda s: s` for readable output.
- `test_applied_status_sets_applied_date`: Insert job, `update_job_status(key, "applied")`. Verify `get_job(key)["applied_date"]` is not None.
- `test_non_applied_status_no_applied_date`: Insert job, `update_job_status(key, "scored")`. Verify `get_job(key)["applied_date"]` is None.
- `test_update_notes`: Insert job, `update_job_notes(key, "Great match for K8s role")`. Verify `get_job(key)["notes"] == "Great match for K8s role"`.
- `test_mark_viewed_sets_timestamp`: Insert job, `mark_viewed(key)`. Verify `get_job(key)["viewed_at"]` is not None.
- `test_mark_viewed_idempotent`: Insert job, `mark_viewed(key)`, record `viewed_at`. Call `mark_viewed(key)` again. Verify `viewed_at` unchanged (SQL condition: `AND viewed_at IS NULL`).
- `test_get_jobs_filter_by_status`: Insert 2 jobs, set one to "applied". Call `get_jobs(status="applied")`. Verify exactly 1 result with correct company.
- `test_get_jobs_filter_by_platform`: Insert 2 jobs on different platforms. Call `get_jobs(platform="dice")`. Verify exactly 1 result.
- `test_get_jobs_filter_by_score_min`: Insert 2 jobs with scores 3 and 5. Call `get_jobs(score_min=4)`. Verify exactly 1 result (score 5).
- `test_get_jobs_sort_order`: Insert 2 jobs with scores 3 and 5. Call `get_jobs(sort_by="score", sort_dir="desc")`. Verify first result has score 5.
- `test_get_jobs_null_scores_last`: Insert 2 jobs, one with `score=5`, one with `score=None`. Call `get_jobs(sort_by="score", sort_dir="desc")`. Verify scored job comes first.

**class TestBulkOperations (DB-04):**

- `test_upsert_jobs_returns_count`: Create list of 5 job dicts. Call `upsert_jobs(jobs)`. Assert return value == 5. Verify each job retrievable via `get_job()`.
- `test_bulk_status_update_changes_target_jobs_only`: Insert 4 jobs. Select 2 keys for update. Loop `update_job_status(key, "saved")` for selected 2. Verify the 2 changed to "saved", the other 2 remain "discovered".
- `test_bulk_status_update_all_jobs`: Insert 3 jobs. Loop `update_job_status(key, "applied")` for all 3. Verify all 3 have status "applied" and all 3 have non-null `applied_date`.
- `test_remove_stale_jobs_preserves_unsearched_platforms`: Insert 4 jobs -- 2 from "indeed", 2 from "dice". Set old `last_seen_at` timestamps on all jobs via direct SQL (e.g., "2026-01-01T00:00:00"). Call `remove_stale_jobs(searched_platforms=["indeed"], run_timestamp="2026-02-08T00:00:00")`. Verify the 2 indeed jobs are deleted (`get_job()` returns None). Verify the 2 dice jobs still exist (dice was not in `searched_platforms`, so its jobs are preserved regardless of timestamp).
- `test_remove_stale_jobs_preserves_recent_jobs`: Insert 2 indeed jobs. Set `last_seen_at` on one to a recent timestamp (same as `run_timestamp`) and the other to an old timestamp. Call `remove_stale_jobs(searched_platforms=["indeed"], run_timestamp="2026-02-08T00:00:00")`. Verify only the stale job is deleted; the recent one remains.

**class TestRunHistory (DB-01 -- supplementary CRUD):**

- `test_record_and_retrieve_run`: Call `record_run(started_at="2026-02-08T10:00:00", finished_at="2026-02-08T10:05:00", mode="scheduled", platforms_searched=["indeed", "dice"], total_raw=50, total_scored=10, new_jobs=5, errors=[], status="success", duration_seconds=300.0)`. Call `get_run_history(limit=10)`. Assert 1 result. Verify fields: mode, status, total_raw, new_jobs, duration_seconds.
- `test_run_history_ordering`: Record 3 runs with different timestamps. Call `get_run_history()`. Verify newest first (ID descending).
- `test_run_history_limit`: Record 5 runs. Call `get_run_history(limit=2)`. Assert exactly 2 returned.

**class TestStats (DB-01 -- supplementary CRUD):**

- `test_get_stats_empty_db`: Call `get_stats()` on empty DB. Verify `total==0`, `by_score=={}`, `by_status=={}`, `by_platform=={}`.
- `test_get_stats_with_jobs`: Insert 3 jobs (indeed score=3, dice score=4, remoteok score=5). Set one to "applied". Call `get_stats()`. Verify total==3, by_platform has 3 keys, by_score has 3 keys, by_status has "discovered" and "applied".
- `test_get_enhanced_stats_structure`: Insert 2 jobs. Call `get_enhanced_stats()`. Verify returned dict has all expected keys: total, jobs_per_day, by_platform, response_rate, time_in_stage, status_funnel. Verify `total==2`.

**class TestBackfillScoreBreakdowns (DB-01 -- supplementary CRUD):**

- `test_backfill_rescores_missing_breakdown`: Insert job with `score=3, score_breakdown=None` (set score via direct SQL after upsert since upsert_job defaults to None). Create mock `scorer_fn = lambda job_dict: (5, {"title": 2, "tech": 2, "remote": 1})`. Call `backfill_score_breakdowns(scorer_fn)`. Verify returns 1. Verify `get_job(key)["score"] == 5` and `get_job(key)["score_breakdown"]` contains the breakdown dict (JSON-encoded).
- `test_backfill_skips_jobs_with_existing_breakdown`: Insert job with `score=4` and `score_breakdown='{"title": 2}'` (set both via direct SQL). Call `backfill_score_breakdowns(scorer_fn)`. Verify returns 0.
- `test_backfill_skips_unscored_jobs`: Insert job with `score=None`. Call `backfill_score_breakdowns(scorer_fn)`. Verify returns 0.

**Important notes for the executor:**
- Import `import webapp.db as db_module` (not individual functions -- match existing test_delta.py pattern).
- The `_fresh_db` autouse fixture from `tests/conftest.py` already provides a clean in-memory DB per test. No additional fixture setup needed.
- For setting score/score_breakdown directly, use `db_module.get_conn().execute("UPDATE jobs SET score = ?, score_breakdown = ? WHERE dedup_key = ?", ...)` followed by `.commit()`.
- For `backfill_score_breakdowns`, the mock `scorer_fn` should be a simple lambda or function, NOT a unittest.mock.Mock (backfill calls it with a dict argument and expects a `(score, breakdown)` tuple).
  </action>
  <verify>
Run: `uv run pytest tests/webapp/test_db.py -v --tb=short -m integration` -- all tests pass.
Run: `uv run pytest tests/webapp/test_db.py -v --tb=short 2>&1 | grep -c "PASSED"` -- expect 25+ passed tests.
  </verify>
  <done>tests/webapp/test_db.py has test classes covering: CRUD lifecycle (insert, read, upsert conflict resolution, all 11 status transitions, applied_date, notes, mark_viewed, filtering, sorting), bulk operations (upsert_jobs count, selective and full bulk status updates), run history (record, ordering, limit), stats (empty, populated, enhanced structure), and backfill (rescore, skip existing, skip unscored). All tests pass with @pytest.mark.integration.</done>
</task>

<task type="auto">
  <name>Task 2: Add schema initialization and migration tests (DB-05)</name>
  <files>tests/webapp/test_db.py</files>
  <action>
Append to the existing `tests/webapp/test_db.py` file created in Task 1. Add schema verification test classes.

**class TestSchemaInitialization (DB-05):**

- `test_all_tables_created`: Query `SELECT name FROM sqlite_master WHERE type='table'`. Assert all expected tables exist: `jobs`, `activity_log`, `run_history`, `resume_versions`, `jobs_fts`.
- `test_all_indexes_created`: Query `SELECT name FROM sqlite_master WHERE type='index'`. Assert all expected indexes exist: `idx_activity_dedup`, `idx_activity_created`, `idx_resume_versions_job`.
- `test_all_triggers_created`: Query `SELECT name FROM sqlite_master WHERE type='trigger'`. Assert all expected triggers exist: `jobs_fts_ai`, `jobs_fts_ad`, `jobs_fts_au`.
- `test_schema_version`: Query `PRAGMA user_version`. Assert value == 6.
- `test_init_db_idempotent`: Call `init_db()` again (already called by `_fresh_db` fixture). Verify no error raised. Query tables again to confirm all still present.
- `test_migrate_db_idempotent`: Call `migrate_db(db_module.get_conn())` on already-migrated DB. Verify no error raised. Verify user_version still == 6.
- `test_jobs_table_columns`: Query `PRAGMA table_info(jobs)`. Verify expected columns exist: id, platform, title, company, location, url, salary, salary_min, salary_max, apply_url, description, posted_date, tags, easy_apply, score, status, applied_date, notes, created_at, updated_at, dedup_key, first_seen_at, last_seen_at, viewed_at, score_breakdown, company_aliases, salary_display, salary_currency. (Assert column count >= 28.)

**Implementation notes:**
- All schema queries use `db_module.get_conn().execute(...)` to query `sqlite_master`.
- For table_info, column names are in `row[1]` (second element of PRAGMA table_info result).
- Keep assertions as sets or sorted lists for order-independent comparison.
  </action>
  <verify>
Run: `uv run pytest tests/webapp/test_db.py::TestSchemaInitialization -v --tb=short` -- all 7 tests pass.
Run: `uv run pytest tests/webapp/test_db.py -v --tb=short` -- all tests from both tasks pass (30+ total).
  </verify>
  <done>TestSchemaInitialization class covers: all 5 expected tables verified against sqlite_master, all 3 expected indexes, all 3 expected triggers, PRAGMA user_version==6, init_db idempotency, migrate_db idempotency, and jobs table column completeness. All tests pass.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/webapp/test_db.py -v --tb=short` -- all tests pass (30+ tests)
2. `uv run pytest tests/webapp/test_db.py -m integration -v` -- all tests are correctly marked as integration
3. `uv run pytest tests/webapp/test_db.py --cov=webapp/db --cov-report=term-missing -q` -- coverage report shows significant coverage of webapp/db.py
4. `uv run pytest tests/ -v --tb=short -q` -- all existing tests still pass (no regressions)
</verification>

<success_criteria>
- tests/webapp/test_db.py has TestCrudLifecycle covering insert, read, upsert conflict, all 11 status transitions, applied_date auto-set, notes, mark_viewed, filtering, sorting
- TestBulkOperations covers upsert_jobs count and selective vs full bulk status update
- TestRunHistory covers record, ordering, and limit
- TestStats covers empty DB, populated DB, and enhanced stats structure
- TestBackfillScoreBreakdowns covers rescore, skip existing, skip unscored
- TestSchemaInitialization covers tables, indexes, triggers, user_version, idempotency, columns
- All tests pass with `uv run pytest tests/webapp/test_db.py -v`
- All tests marked with `@pytest.mark.integration`
</success_criteria>

<output>
After completion, create `.planning/phases/11-database-integration-tests/11-01-SUMMARY.md`
</output>
