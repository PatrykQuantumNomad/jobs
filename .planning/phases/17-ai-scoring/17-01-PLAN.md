---
phase: 17-ai-scoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - ai_scorer.py
  - webapp/db.py
  - tests/test_ai_scorer.py
autonomous: true

must_haves:
  truths:
    - "AI scorer function accepts resume text + job description and returns validated AIScoreResult(score, reasoning, strengths, gaps)"
    - "Database has ai_score, ai_score_breakdown, ai_scored_at columns on jobs table after migration v7"
    - "update_ai_score() stores AI score results and logs activity"
    - "AI scorer tests pass with mocked CLI subprocess (no real CLI calls)"
  artifacts:
    - path: "ai_scorer.py"
      provides: "AIScoreResult model + score_job_ai() async function + SYSTEM_PROMPT"
      exports: ["AIScoreResult", "score_job_ai"]
    - path: "webapp/db.py"
      provides: "Migration v7 with AI score columns + update_ai_score function"
      contains: "ai_score INTEGER"
    - path: "tests/test_ai_scorer.py"
      provides: "Unit tests for AI scorer module"
      min_lines: 50
  key_links:
    - from: "ai_scorer.py"
      to: "claude_cli.run()"
      via: "async call with AIScoreResult output_model"
      pattern: "cli_run.*output_model=AIScoreResult"
    - from: "ai_scorer.py"
      to: "CLIError -> RuntimeError"
      via: "exception wrapping at boundary"
      pattern: "except CLIError.*raise RuntimeError"
    - from: "webapp/db.py"
      to: "jobs table"
      via: "ALTER TABLE migration v7"
      pattern: "ALTER TABLE jobs ADD COLUMN ai_score"
---

<objective>
Build the AI scoring backend: Pydantic model for structured CLI output, async scorer function using claude_cli.run(), database migration adding AI score columns, and db helper to persist results.

Purpose: Provides the core scoring logic and storage layer that the dashboard endpoint (plan 17-02) will wire together.
Output: ai_scorer.py module, db migration v7, update_ai_score() function, unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-ai-scoring/17-RESEARCH.md
@.planning/phases/16-cli-wrapper-foundation/16-02-SUMMARY.md

@resume_ai/tailor.py
@claude_cli/client.py
@claude_cli/exceptions.py
@webapp/db.py
@tests/resume_ai/conftest.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ai_scorer.py with AIScoreResult model and score_job_ai function</name>
  <files>ai_scorer.py</files>
  <action>
Create `ai_scorer.py` at the project root (same level as `scorer.py`). This is a single-module scorer, NOT a package -- the feature is small enough.

**AIScoreResult Pydantic model:**
- `score: int = Field(ge=1, le=5)` with description about overall job-fit
- `reasoning: str = Field(...)` with description requesting 2-3 sentences
- `strengths: list[str] = Field(...)` requesting 3-5 concrete skills from resume
- `gaps: list[str] = Field(...)` requesting 0-5 specific requirements from job description
- Use exact Field descriptions from research (they become the JSON schema that guides the LLM)

**SYSTEM_PROMPT constant:**
- Use the exact prompt from research section "System Prompt for AI Scoring" (scoring rubric 1-5, evaluation criteria, important rules)
- This is a raw string constant at module level, no function needed

**score_job_ai() async function:**
- Signature: `async def score_job_ai(resume_text: str, job_description: str, job_title: str, company_name: str, model: str = "sonnet") -> AIScoreResult`
- Build user_message with sections: "## Candidate Resume", "## Job Description", "## Target Role" (with job_title and company_name)
- Call `await cli_run(system_prompt=SYSTEM_PROMPT, user_message=user_message, output_model=AIScoreResult, model=model)`
- Wrap `CLIError` in `RuntimeError` per 16-02 pattern: `except CLIError as exc: raise RuntimeError(f"AI scoring failed: {exc}") from exc`

**Imports:**
- `from claude_cli import run as cli_run`
- `from claude_cli.exceptions import CLIError`
- `from pydantic import BaseModel, Field`

Follow the exact pattern from `resume_ai/tailor.py` -- the function signature, import style, docstring format, and error wrapping are identical.
  </action>
  <verify>
`uv run python -c "from ai_scorer import AIScoreResult, score_job_ai; print(AIScoreResult.model_json_schema())"` prints valid JSON schema with score constrained to 1-5.
`uv run ruff check ai_scorer.py` passes with no errors.
  </verify>
  <done>ai_scorer.py exists with AIScoreResult (score 1-5, reasoning, strengths, gaps), SYSTEM_PROMPT, and async score_job_ai() that calls cli_run() and wraps CLIError in RuntimeError.</done>
</task>

<task type="auto">
  <name>Task 2: Add database migration v7 and update_ai_score function</name>
  <files>webapp/db.py</files>
  <action>
Modify `webapp/db.py` to add AI score storage.

**1. Increment SCHEMA_VERSION from 6 to 7** (line 20).

**2. Add migration v7 to MIGRATIONS dict** (after the version 6 entry):
```python
7: [
    "ALTER TABLE jobs ADD COLUMN ai_score INTEGER",
    "ALTER TABLE jobs ADD COLUMN ai_score_breakdown TEXT",
    "ALTER TABLE jobs ADD COLUMN ai_scored_at TEXT",
],
```
These columns store: the 1-5 score, the full breakdown as JSON (reasoning + strengths + gaps), and the ISO timestamp of when scoring happened.

**3. Add update_ai_score() function** after the existing `update_job_notes()` function (around line 490):
```python
def update_ai_score(dedup_key: str, score: int, breakdown: dict) -> None:
    """Store AI score results for a job."""
    now = datetime.now().isoformat()
    with get_conn() as conn:
        conn.execute(
            """UPDATE jobs
               SET ai_score = ?, ai_score_breakdown = ?, ai_scored_at = ?, updated_at = ?
               WHERE dedup_key = ?""",
            (score, json.dumps(breakdown), now, now, dedup_key),
        )
    log_activity(dedup_key, "ai_scored", new_value=str(score))
```

The function takes the raw score integer and breakdown dict, serializes breakdown to JSON, sets the timestamp, and logs an activity event. This follows the exact pattern of `update_job_status()` and `update_job_notes()`.

Do NOT modify the `get_job()` function -- it already returns `SELECT *` which will include the new columns automatically.
  </action>
  <verify>
`uv run pytest tests/ -x -q` -- all existing tests pass (migration is idempotent, _fresh_db re-runs init_db which runs migrations).
`uv run python -c "from webapp import db; job = db.get_job('nonexistent'); print('ok')"` confirms the module loads without error.
`uv run ruff check webapp/db.py` passes.
  </verify>
  <done>SCHEMA_VERSION is 7, migration v7 adds ai_score/ai_score_breakdown/ai_scored_at columns, update_ai_score() persists score + logs activity, all existing tests pass.</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for ai_scorer module</name>
  <files>tests/test_ai_scorer.py</files>
  <action>
Create `tests/test_ai_scorer.py` with tests for the AI scorer module. Use the `mock_claude_cli` fixture from `tests/resume_ai/conftest.py` (it's importable via conftest discovery since tests/ is the root).

NOTE: The `mock_claude_cli` fixture lives in `tests/resume_ai/conftest.py`. For it to be available in `tests/test_ai_scorer.py` (which is at the tests/ root level, NOT inside tests/resume_ai/), you need to EITHER:
- Move the `mock_claude_cli` fixture to `tests/conftest.py` (the root conftest), OR
- Copy the fixture into `tests/test_ai_scorer.py` as a local fixture, OR
- Create a `tests/conftest_cli.py` and use `pytest_plugins` to load it

**Best approach:** Move the `mock_claude_cli` fixture from `tests/resume_ai/conftest.py` to `tests/conftest.py` (the root conftest). This makes it available to ALL test files. The `tests/resume_ai/conftest.py` file can then be simplified or removed. Update `tests/resume_ai/conftest.py` to remove the fixture (it will be inherited from parent conftest). This is a clean refactor since the fixture is useful project-wide, not just for resume_ai tests.

**Test class: TestAIScorer** (mark with `@pytest.mark.unit`)

Tests to write:

1. `test_score_job_ai_success` -- Set mock_claude_cli response to a valid AIScoreResult instance (score=4, reasoning="Good match...", strengths=["Python", "AWS"], gaps=["Kubernetes"]). Call `await score_job_ai(resume_text="...", job_description="...", job_title="Engineer", company_name="Acme")`. Assert result.score == 4, result.reasoning contains expected text, len(result.strengths) == 2, len(result.gaps) == 1.

2. `test_score_job_ai_cli_error_wraps_in_runtime_error` -- Set mock_claude_cli error (returncode=1, stderr="auth failed"). Call score_job_ai and assert it raises RuntimeError with "AI scoring failed" in the message. Use `pytest.raises(RuntimeError, match="AI scoring failed")`.

3. `test_ai_score_result_validates_score_range` -- Test that `AIScoreResult(score=0, ...)` raises ValidationError. Test that `AIScoreResult(score=6, ...)` raises ValidationError. Test that `AIScoreResult(score=3, ...)` succeeds.

4. `test_ai_score_result_schema_has_constraints` -- Call `AIScoreResult.model_json_schema()` and verify the schema has score minimum=1, maximum=5.

**Test class: TestUpdateAIScore** (mark with `@pytest.mark.integration`)

5. `test_update_ai_score_persists` -- Insert a job via `db.upsert_job(...)`, call `db.update_ai_score(dedup_key, score=4, breakdown={"reasoning": "test", "strengths": ["a"], "gaps": ["b"]})`, then `db.get_job(dedup_key)` and assert ai_score == 4, ai_score_breakdown is a JSON string containing "reasoning", ai_scored_at is not None.

6. `test_update_ai_score_logs_activity` -- After calling update_ai_score, check `db.get_activity_log(dedup_key)` contains an event with event_type="ai_scored" and new_value="4".

All async tests need `@pytest.mark.asyncio` decorator. Import pytest_asyncio or use `pytest.mark.asyncio` (the project already uses pytest-asyncio based on the existing resume_ai tests).
  </action>
  <verify>
`uv run pytest tests/test_ai_scorer.py -v` -- all 6 tests pass.
`uv run pytest tests/resume_ai/ -v` -- existing resume_ai tests still pass (mock_claude_cli moved but still available).
`uv run ruff check tests/test_ai_scorer.py` passes.
  </verify>
  <done>6 tests covering: successful AI scoring, CLI error wrapping, Pydantic validation range, schema constraints, database persistence, and activity logging. All pass with mocked CLI. Existing tests unbroken.</done>
</task>

</tasks>

<verification>
1. `uv run ruff check ai_scorer.py webapp/db.py tests/test_ai_scorer.py` -- zero lint errors
2. `uv run pytest tests/test_ai_scorer.py -v` -- all AI scorer tests pass
3. `uv run pytest tests/ -x -q` -- full test suite passes (no regressions from migration or fixture move)
4. `uv run python -c "from ai_scorer import AIScoreResult; print(AIScoreResult.model_json_schema())"` -- schema shows score ge=1 le=5
</verification>

<success_criteria>
- ai_scorer.py exists at project root with AIScoreResult model, SYSTEM_PROMPT, and async score_job_ai()
- webapp/db.py has SCHEMA_VERSION=7, migration v7 adding 3 AI score columns, update_ai_score() function
- tests/test_ai_scorer.py has 6 passing tests covering scorer logic, validation, db persistence, and error handling
- Full test suite passes with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/17-ai-scoring/17-01-SUMMARY.md`
</output>
