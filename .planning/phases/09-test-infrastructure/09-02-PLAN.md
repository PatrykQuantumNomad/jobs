---
phase: 09-test-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - tests/conftest.py
  - tests/conftest_factories.py
  - tests/webapp/conftest.py
  - tests/platforms/conftest.py
  - tests/resume_ai/conftest.py
  - tests/test_smoke.py
autonomous: true

must_haves:
  truths:
    - "Every test starts with a clean settings singleton -- calling get_settings() in test A does not affect test B"
    - "Tests using the database fixture operate on a fresh in-memory SQLite instance, never touching job_pipeline/jobs.db"
    - "Any test that accidentally calls the Anthropic API fails immediately with a descriptive RuntimeError"
    - "Any test that accidentally opens a real network socket fails immediately with SocketBlockedError"
    - "JobFactory() produces a valid Job instance that passes Pydantic validation including cross-field constraints"
    - "db_with_jobs fixture returns a list of Job instances and the database contains matching rows"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Global autouse fixtures: settings reset, fresh DB, Anthropic guard; opt-in fixtures: db_with_jobs"
      contains: "JOBFLOW_TEST_DB"
    - path: "tests/conftest_factories.py"
      provides: "Factory Boy factories for Job model with Pydantic v2 compatibility"
      contains: "class JobFactory"
    - path: "tests/webapp/conftest.py"
      provides: "FastAPI TestClient fixture using in-memory DB"
      contains: "TestClient"
    - path: "tests/platforms/conftest.py"
      provides: "Platform-specific test fixtures (mock RemoteOK API)"
      contains: "mock_remoteok_api"
    - path: "tests/resume_ai/conftest.py"
      provides: "Mock Anthropic client fixture for AI module tests"
      contains: "mock_anthropic"
    - path: "tests/test_smoke.py"
      provides: "Smoke tests validating the test infrastructure itself"
      contains: "test_factory_produces_valid_job"
  key_links:
    - from: "tests/conftest.py"
      to: "config.py"
      via: "reset_settings() call in autouse fixture"
      pattern: "from config import reset_settings"
    - from: "tests/conftest.py"
      to: "webapp/db.py"
      via: "_memory_conn reset in autouse fixture"
      pattern: "db_module._memory_conn"
    - from: "tests/conftest_factories.py"
      to: "models.py"
      via: "Factory Meta.model = Job"
      pattern: "model = Job"
    - from: "tests/conftest.py"
      to: "anthropic SDK"
      via: "monkeypatch block on Messages.create"
      pattern: "anthropic.resources.messages.Messages.create"
---

<objective>
Create all conftest.py files with isolation fixtures, factory definitions, and sub-directory fixtures. Validate the entire infrastructure with smoke tests.

Purpose: This is the core of Phase 9 -- the fixtures and guards that every subsequent test phase depends on. Settings isolation prevents config leakage, per-test fresh DB prevents data leakage, socket blocking prevents accidental network calls, and the Anthropic guard prevents accidental LLM charges.

Output: Root conftest.py with 3 autouse fixtures + 1 opt-in fixture; conftest_factories.py with JobFactory; 3 sub-directory conftest files; test_smoke.py validating it all works.
</objective>

<execution_context>
@/Users/patrykattc/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrykattc/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-test-infrastructure/09-RESEARCH.md
@.planning/phases/09-test-infrastructure/09-01-SUMMARY.md
@config.py
@models.py
@webapp/db.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create root conftest.py with isolation fixtures and conftest_factories.py</name>
  <files>
tests/conftest.py
tests/conftest_factories.py
  </files>
  <action>
Create the two core files that provide global test isolation and data factories.

**tests/conftest.py** -- Module docstring explaining the critical import ordering. Structure:

1. **Environment setup (BEFORE any project imports):**
   ```python
   os.environ["JOBFLOW_TEST_DB"] = "1"
   os.environ["ANTHROPIC_API_KEY"] = "test-key-not-real"
   ```
   These MUST be the very first executable lines after the `import os` statement. `JOBFLOW_TEST_DB` prevents `webapp/db.py` from creating a file-based database when it runs `init_db()` at import time (line 723). `ANTHROPIC_API_KEY` prevents `AuthenticationError` when the Anthropic client is instantiated.

2. **Imports (after env setup):**
   ```python
   import pytest  # noqa: E402
   from config import reset_settings  # noqa: E402
   import webapp.db as db_module  # noqa: E402
   ```
   Use `# noqa: E402` on each import since ruff will flag imports after `os.environ` assignments.

3. **Autouse fixture: `_reset_settings`** -- calls `reset_settings()` before AND after each test. This clears the `_settings` singleton in `config.py`, preventing config leakage between tests.

4. **Autouse fixture: `_fresh_db`** -- before each test: close `db_module._memory_conn` if not None, set it to None, then call `db_module.init_db()` to create a fresh in-memory DB with full schema. After test (in yield): close and set to None again. This guarantees zero data leakage between tests.

5. **Autouse fixture: `_block_anthropic(monkeypatch)`** -- patches `anthropic.resources.messages.Messages.create` and `anthropic.resources.messages.Messages.parse` to raise `RuntimeError` with a descriptive message: `"Test attempted real Anthropic API call to api.anthropic.com -- use the mock_anthropic fixture instead"`. Wrap in `try/except ImportError` in case anthropic isn't installed.

6. **Opt-in fixture: `db_with_jobs(_fresh_db)`** -- depends on `_fresh_db` (explicit). Imports `JobFactory` from `tests.conftest_factories`. Creates 9 jobs (3 per platform: indeed, dice, remoteok) with scores cycling 3, 4, 5. Plus 1 high-scoring job (score=5, title="Principal Engineer", platform="indeed"). Upserts all 10 into DB via `db_module.upsert_job(job.model_dump(mode="json"))`. Returns the list of 10 Job instances.

**tests/conftest_factories.py** -- Factory Boy definitions:

```python
"""Factory Boy factories for Pydantic v2 models.

factory-boy has no native Pydantic integration, but setting
``class Meta: model = Job`` works because factory-boy calls
``Job(field1=val1, ...)`` which triggers Pydantic validation.
"""

import factory
from faker import Faker

from models import Job, JobStatus

fake = Faker()


class JobFactory(factory.Factory):
    """Factory producing valid models.Job instances.

    All field values satisfy Pydantic constraints:
    - platform: cycles through valid Literal values
    - score: 1-5 (ge=1, le=5)
    - salary_max >= salary_min (cross-field validator)
    - description: str (not list -- Faker's paragraphs returns list)
    """

    class Meta:
        model = Job

    id = factory.LazyFunction(lambda: fake.hexify("????????????????"))
    platform = factory.Iterator(["indeed", "dice", "remoteok"])
    title = factory.Faker("job")
    company = factory.Faker("company")
    location = factory.LazyFunction(
        lambda: fake.random_element(["Remote", "New York, NY", "Toronto, ON"])
    )
    url = factory.LazyFunction(
        lambda: f"https://example.com/jobs/{fake.uuid4()}"
    )
    salary = factory.LazyFunction(
        lambda: f"${fake.random_int(150, 300)}K"
    )
    salary_min = factory.LazyFunction(
        lambda: fake.random_int(150000, 200000)
    )
    # salary_max must be >= salary_min (Pydantic validator)
    salary_max = factory.LazyAttribute(
        lambda obj: obj.salary_min + fake.random_int(0, 150000)
    )
    posted_date = factory.LazyFunction(
        lambda: fake.date_between("-14d", "today").isoformat()
    )
    tags = factory.LazyFunction(
        lambda: fake.random_elements(
            ["python", "kubernetes", "terraform", "docker", "aws", "gcp"],
            unique=True,
            length=3,
        )
    )
    easy_apply = factory.Faker("boolean")
    score = factory.LazyFunction(lambda: fake.random_int(1, 5))
    status = JobStatus.SCORED

    @factory.lazy_attribute
    def description(self):
        """Join paragraphs into a single string (Faker's paragraphs returns list)."""
        return "\n\n".join(fake.paragraphs(nb=3))
```

Note: The `description` field uses `@factory.lazy_attribute` which overrides the class-level attribute. This is the correct pattern -- the `@factory.lazy_attribute` takes precedence.
  </action>
  <verify>
Run: `uv run python -c "
import os; os.environ['JOBFLOW_TEST_DB'] = '1'; os.environ['ANTHROPIC_API_KEY'] = 'test'
from tests.conftest_factories import JobFactory
job = JobFactory()
print(f'Factory works: {job.title} at {job.company}, score={job.score}, platform={job.platform}')
assert job.salary_max >= job.salary_min, 'salary_max must be >= salary_min'
print('Cross-field validation passed')
"` -- should print job details and validation pass.
  </verify>
  <done>Root conftest.py has 3 autouse fixtures (settings reset, fresh DB, Anthropic guard) and 1 opt-in fixture (db_with_jobs). conftest_factories.py has JobFactory that produces valid Pydantic v2 Job instances with correct cross-field constraints.</done>
</task>

<task type="auto">
  <name>Task 2: Create sub-directory conftest files and smoke tests</name>
  <files>
tests/webapp/conftest.py
tests/platforms/conftest.py
tests/resume_ai/conftest.py
tests/test_smoke.py
  </files>
  <action>
Create domain-specific conftest files for each test sub-directory, plus smoke tests that validate the entire infrastructure.

**tests/webapp/conftest.py** -- FastAPI TestClient fixture:
```python
"""Webapp test fixtures -- FastAPI TestClient and DB seeding."""

import pytest
from fastapi.testclient import TestClient

from webapp.app import app


@pytest.fixture
def client():
    """FastAPI test client backed by the in-memory DB from _fresh_db."""
    return TestClient(app)
```

**tests/platforms/conftest.py** -- Mock RemoteOK API fixture:
```python
"""Platform test fixtures -- mocked API responses."""

import pytest
import httpx
import respx


@pytest.fixture
def mock_remoteok_api():
    """Mock the RemoteOK API with a realistic sample response.

    Index 0 is metadata (skipped by parser). Index 1+ are jobs.
    """
    sample_response = [
        {"legal": "https://remoteok.com/legal"},
        {
            "id": 12345,
            "slug": "test-senior-platform-engineer",
            "epoch": "1707350400",
            "date": "2026-02-08T00:00:00+00:00",
            "company": "TestCorp",
            "company_logo": "https://example.com/logo.png",
            "position": "Senior Platform Engineer",
            "tags": ["python", "kubernetes", "docker"],
            "description": "<p>We need a platform engineer with Kubernetes experience.</p>",
            "location": "Remote",
            "salary_min": 200000,
            "salary_max": 300000,
            "apply_url": "https://testcorp.com/careers/12345",
            "url": "https://remoteok.com/remote-jobs/12345",
        },
        {
            "id": 12346,
            "slug": "test-devops-lead",
            "epoch": "1707264000",
            "date": "2026-02-07T00:00:00+00:00",
            "company": "AnotherCo",
            "company_logo": "",
            "position": "DevOps Lead",
            "tags": ["terraform", "aws", "python"],
            "description": "<p>Lead our DevOps team.</p>",
            "location": "Worldwide",
            "salary_min": 180000,
            "salary_max": 250000,
            "apply_url": "https://anotherco.com/apply",
            "url": "https://remoteok.com/remote-jobs/12346",
        },
    ]
    with respx.mock:
        respx.get("https://remoteok.com/api").mock(
            return_value=httpx.Response(200, json=sample_response)
        )
        yield sample_response
```

**tests/resume_ai/conftest.py** -- Mock Anthropic client fixture:
```python
"""Resume AI test fixtures -- mocked Anthropic client."""

from unittest.mock import MagicMock

import pytest


@pytest.fixture
def mock_anthropic(monkeypatch):
    """Provide a mock Anthropic client that returns controlled responses.

    Overrides the autouse _block_anthropic guard for tests that need
    to simulate LLM responses.

    Usage in tests:
        def test_tailor(mock_anthropic):
            mock_anthropic.messages.create.return_value = ...
    """
    mock_client = MagicMock()

    # Default response structure matching Anthropic SDK
    mock_response = MagicMock()
    mock_response.content = [MagicMock(text="Mock AI response")]
    mock_response.usage.input_tokens = 100
    mock_response.usage.output_tokens = 50
    mock_client.messages.create.return_value = mock_response

    try:
        import anthropic
        monkeypatch.setattr(anthropic, "Anthropic", lambda **kw: mock_client)
    except ImportError:
        pass

    return mock_client
```

**tests/test_smoke.py** -- Smoke tests validating the test infrastructure:
```python
"""Smoke tests for test infrastructure.

These tests verify that the fixtures and isolation guards from conftest.py
work correctly. They should be the first tests to pass in the suite.
"""

import os

import pytest

from models import Job, JobStatus


class TestFactorySmoke:
    """Verify JobFactory produces valid Pydantic models."""

    def test_factory_produces_valid_job(self):
        from tests.conftest_factories import JobFactory
        job = JobFactory()
        assert isinstance(job, Job)
        assert job.platform in ("indeed", "dice", "remoteok")
        assert 1 <= job.score <= 5
        assert isinstance(job.description, str)
        assert len(job.description) > 0

    def test_factory_salary_constraint(self):
        from tests.conftest_factories import JobFactory
        for _ in range(20):  # Run multiple times to catch random failures
            job = JobFactory()
            if job.salary_min is not None and job.salary_max is not None:
                assert job.salary_max >= job.salary_min

    def test_factory_override(self):
        from tests.conftest_factories import JobFactory
        job = JobFactory(
            platform="dice",
            title="Staff Engineer",
            score=5,
            company="TestCorp",
        )
        assert job.platform == "dice"
        assert job.title == "Staff Engineer"
        assert job.score == 5
        assert job.company == "TestCorp"


class TestSettingsIsolation:
    """Verify settings singleton resets between tests."""

    def test_settings_clean_a(self):
        from config import get_settings, _settings
        # _settings should be None at the start of each test
        # (autouse _reset_settings clears it)
        from config import _settings as current
        assert current is None

    def test_settings_clean_b(self):
        # Even after test_a called get_settings, test_b should start clean
        from config import _settings as current
        assert current is None


class TestDatabaseIsolation:
    """Verify each test gets a fresh in-memory database."""

    def test_db_is_empty(self):
        import webapp.db as db_module
        conn = db_module.get_conn()
        cursor = conn.execute("SELECT COUNT(*) FROM jobs")
        count = cursor.fetchone()[0]
        assert count == 0, "Fresh DB should have no jobs"

    def test_db_insert_does_not_leak(self):
        import webapp.db as db_module
        # Insert a job
        db_module.upsert_job({
            "id": "test-leak",
            "platform": "indeed",
            "title": "Leak Test",
            "company": "LeakCo",
            "url": "https://example.com/leak",
            "status": "scored",
        })
        conn = db_module.get_conn()
        cursor = conn.execute("SELECT COUNT(*) FROM jobs")
        assert cursor.fetchone()[0] == 1

    def test_db_previous_insert_not_visible(self):
        # The job inserted in test_db_insert_does_not_leak should NOT be here
        import webapp.db as db_module
        conn = db_module.get_conn()
        cursor = conn.execute("SELECT COUNT(*) FROM jobs")
        count = cursor.fetchone()[0]
        assert count == 0, "Previous test's data should not leak"


class TestDbWithJobsFixture:
    """Verify the db_with_jobs fixture works."""

    def test_seeded_db(self, db_with_jobs):
        import webapp.db as db_module
        conn = db_module.get_conn()
        cursor = conn.execute("SELECT COUNT(*) FROM jobs")
        count = cursor.fetchone()[0]
        assert count == len(db_with_jobs)
        assert count == 10  # 3 platforms * 3 + 1 high-scoring


class TestAnthropicGuard:
    """Verify Anthropic API calls are blocked."""

    def test_anthropic_blocked(self):
        try:
            import anthropic
            client = anthropic.Anthropic()
            with pytest.raises(RuntimeError, match="real Anthropic API call"):
                client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=10,
                    messages=[{"role": "user", "content": "test"}],
                )
        except ImportError:
            pytest.skip("anthropic not installed")


class TestNetworkBlocked:
    """Verify real network access is blocked by pytest-socket."""

    def test_socket_blocked(self):
        import socket
        with pytest.raises(Exception):
            # pytest-socket raises SocketBlockedError which is a subclass of Exception
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.connect(("1.1.1.1", 80))


class TestEnvironment:
    """Verify test environment is correctly configured."""

    def test_jobflow_test_db_set(self):
        assert os.environ.get("JOBFLOW_TEST_DB") == "1"

    def test_anthropic_key_is_fake(self):
        assert os.environ.get("ANTHROPIC_API_KEY") == "test-key-not-real"
```

IMPORTANT: The smoke test classes use plain `class TestXxx` (no pytest.fixture usage in some), and import lazily within test methods where needed to ensure proper fixture ordering. The settings isolation tests check `_settings` directly (import the module-level variable) to prove the autouse fixture is working.
  </action>
  <verify>
Run: `uv run pytest tests/test_smoke.py -v` -- all smoke tests should pass.
Run: `uv run pytest tests/test_smoke.py -v --tb=short 2>&1 | tail -20` -- look for "passed" count matching expected.
Run: `ls -la job_pipeline/jobs.db 2>/dev/null && echo "FAIL: file DB created" || echo "OK: no file DB"` -- must NOT have created a file-based database.
  </verify>
  <done>All 3 sub-directory conftest files exist with domain-specific fixtures (TestClient, mock_remoteok_api, mock_anthropic). Smoke tests validate: factory produces valid jobs, settings isolation works, DB isolation works, db_with_jobs seeds 10 jobs, Anthropic guard blocks real calls, network sockets are blocked, environment vars are correct. All tests pass with `uv run pytest tests/test_smoke.py -v`.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_smoke.py -v` -- all smoke tests pass (expect 12+ tests)
2. `uv run pytest --collect-only` -- discovers test_smoke.py, shows correct markers
3. `ls job_pipeline/jobs.db 2>/dev/null` -- must NOT exist (in-memory DB only)
4. `uv run pytest tests/test_smoke.py --cov -q` -- coverage report generates without errors
5. Review conftest.py: `JOBFLOW_TEST_DB=1` is set before ANY project import
6. Review conftest_factories.py: salary_max uses LazyAttribute depending on salary_min
</verification>

<success_criteria>
- tests/conftest.py has 3 autouse fixtures (_reset_settings, _fresh_db, _block_anthropic) and 1 opt-in (db_with_jobs)
- JOBFLOW_TEST_DB=1 and ANTHROPIC_API_KEY are set at top of conftest.py before all project imports
- tests/conftest_factories.py has JobFactory with salary_max >= salary_min guarantee and str description
- tests/webapp/conftest.py has TestClient fixture
- tests/platforms/conftest.py has mock_remoteok_api fixture with realistic sample data
- tests/resume_ai/conftest.py has mock_anthropic fixture that overrides the autouse guard
- tests/test_smoke.py validates all infrastructure: factory, settings isolation, DB isolation, seeded DB, Anthropic guard, network blocking, environment vars
- All smoke tests pass: `uv run pytest tests/test_smoke.py -v`
- No file-based database created during test run
</success_criteria>

<output>
After completion, create `.planning/phases/09-test-infrastructure/09-02-SUMMARY.md`
</output>
