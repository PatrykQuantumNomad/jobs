---
phase: 15-e2e-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - tests/e2e/__init__.py
  - tests/e2e/conftest.py
  - tests/e2e/test_dashboard.py
  - tests/e2e/test_status.py
autonomous: true

must_haves:
  truths:
    - "Dashboard loads in a Playwright browser and displays a list of jobs from the test database"
    - "Filtering by score, platform, and status in the browser UI returns the correct subset of jobs"
    - "Changing a job's status via the UI persists the change -- reloading the page shows the updated status"
  artifacts:
    - path: "tests/e2e/__init__.py"
      provides: "E2E test package marker"
    - path: "tests/e2e/conftest.py"
      provides: "live_server fixture, seeded_db fixture, browser_context_args fixture"
      exports: ["live_server", "seeded_db", "browser_context_args"]
    - path: "tests/e2e/test_dashboard.py"
      provides: "Dashboard load and filtering E2E tests"
      contains: "class TestDashboardE2E"
    - path: "tests/e2e/test_status.py"
      provides: "Status change persistence E2E test"
      contains: "class TestStatusUpdateE2E"
  key_links:
    - from: "tests/e2e/conftest.py"
      to: "webapp.app"
      via: "uvicorn.Server running app in daemon thread"
      pattern: "from webapp.app import app"
    - from: "tests/e2e/conftest.py"
      to: "webapp.db"
      via: "shared _memory_conn singleton (check_same_thread=False)"
      pattern: "db_module\\.upsert_job"
    - from: "tests/e2e/test_dashboard.py"
      to: "tests/e2e/conftest.py"
      via: "live_server and seeded_db fixtures"
      pattern: "live_server.*seeded_db"
---

<objective>
Create E2E test infrastructure and tests for dashboard load, filtering, and status change.

Purpose: Establish the live server fixture pattern and verify the three most fundamental dashboard user flows (E2E-01, E2E-02, E2E-03) work end-to-end in a real browser against a real FastAPI server with a seeded SQLite database.

Output: `tests/e2e/` package with conftest fixtures and two test files covering dashboard display, filtering, and status persistence.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-e2e-tests/15-RESEARCH.md

@pyproject.toml
@tests/conftest.py
@tests/conftest_factories.py
@webapp/app.py
@webapp/db.py
@webapp/templates/dashboard.html
@webapp/templates/partials/job_rows.html
@webapp/templates/job_detail.html
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add pytest-playwright dependency and create E2E conftest with live server fixture</name>
  <files>pyproject.toml, tests/e2e/__init__.py, tests/e2e/conftest.py</files>
  <action>
1. Add `"pytest-playwright>=0.6.0"` to the `[dependency-groups] dev` list in `pyproject.toml`. Run `uv sync --dev` to install it.

2. Create `tests/e2e/__init__.py` (empty file).

3. Create `tests/e2e/conftest.py` with these fixtures:

**`live_server` (scope="session"):**
- Import `from webapp.app import app` and `from uvicorn import Config, Server`
- Start uvicorn on `127.0.0.1:8765` (non-standard port to avoid conflicts) with `log_level="warning"` in a daemon thread
- Poll `_port_is_open()` with 10-second deadline using `socket.connect_ex()`
- Yield `f"http://127.0.0.1:8765"`
- On teardown: `server.should_exit = True; thread.join(timeout=5)`

**`browser_context_args` (scope="session"):**
- Override pytest-playwright's fixture to add `accept_downloads: True` and `viewport: {"width": 1280, "height": 800}`

**`seeded_db` (function-scoped, depends on `_fresh_db`):**
- Import `JobFactory` from `tests.conftest_factories` and `webapp.db` as `db_module`
- Create 9 jobs: 3 per platform (indeed, dice, remoteok) with scores cycling 3, 4, 5, all with status `"scored"`
- Create 1 additional "saved" status job for status update testing: `JobFactory(platform="indeed", score=4, title="Saved Test Job", status=JobStatus.SAVED)`
- Upsert all via `db_module.upsert_job(job.model_dump(mode="json"))`
- For the "saved" job, also call `db_module.update_job_status(dedup_key, "saved")` to ensure activity log records it
- Return the list of all 10 jobs

**`_ensure_htmx` (function-scoped, autouse within e2e module):**
- After each `page.goto()` Playwright auto-waits, but to guard against CDN issues, add a fixture that is a no-op for now (placeholder comment explaining you can add `page.wait_for_function("typeof htmx !== 'undefined'")` if CDN loading becomes flaky)
- Actually, do NOT make this autouse. Just add a comment in conftest explaining the CDN dependency.

All fixtures should have clear docstrings. Add `from __future__ import annotations` at top of file.
  </action>
  <verify>
Run `uv sync --dev` succeeds. Run `uv run python -c "import pytest_playwright; print('OK')"` prints OK. Verify `tests/e2e/conftest.py` exists and imports cleanly: `uv run python -c "import tests.e2e.conftest"`.
  </verify>
  <done>pytest-playwright is installed, tests/e2e/ package exists with conftest.py containing live_server, browser_context_args, and seeded_db fixtures.</done>
</task>

<task type="auto">
  <name>Task 2: Write dashboard load and filtering E2E tests (E2E-01, E2E-02)</name>
  <files>tests/e2e/test_dashboard.py</files>
  <action>
Create `tests/e2e/test_dashboard.py` with class `TestDashboardE2E` decorated with `@pytest.mark.e2e` and `@pytest.mark.enable_socket`.

Add `from __future__ import annotations` and `from playwright.sync_api import expect`.

**Test 1: `test_dashboard_loads_with_jobs` (E2E-01)**
- `page.goto(f"{live_server}/")` using the `live_server` and `seeded_db` fixtures
- Assert page title is `"Dashboard -- Job Tracker"` (note the em-dash from template `&mdash;`)... actually check the template: it uses `—` which is a literal em-dash. Use `expect(page).to_have_title(re.compile("Dashboard"))` or the exact string from the template which is `"Dashboard — Job Tracker"`.
- Assert nav is visible: `expect(page.locator("nav")).to_be_visible()`
- Assert "Total Jobs" stat card text is visible
- Assert job table body has rows: `rows = page.locator("#job-table-body tr.job-row"); expect(rows.first).to_be_visible(); assert rows.count() == 10` (9 scored + 1 saved = 10 jobs from seeded_db)

**Test 2: `test_filter_by_platform` (E2E-02)**
- Navigate to dashboard, verify all 10 rows visible
- Select "indeed" in the platform `<select name="platform">` dropdown using `page.select_option('select[name="platform"]', "indeed")`
- Click the "Filter" button: `page.click('button:has-text("Filter")')`
- Wait for page load: `page.wait_for_load_state("networkidle")`
- Verify filtered results: all visible rows should contain "indeed" platform badge. The seeded_db creates 3 indeed scored + 1 indeed saved = 4 indeed jobs. Assert `rows.count() == 4`.

**Test 3: `test_filter_by_min_score` (E2E-02)**
- Navigate to dashboard
- Select score "4" in `select[name="score"]` (meaning 4+)
- Click Filter, wait for networkidle
- Assert all visible rows have score >= 4. The seeded_db has 3 score-4 jobs + 4 score-5 jobs (3 from cycling + 1 saved with score=4) = actually: 3 platforms x 1 score-4 each = 3, 3 platforms x 1 score-5 each = 3, plus 1 saved job (score=4) = 7 jobs with score >= 4. Assert `rows.count() == 7`.

**Test 4: `test_filter_by_status` (E2E-02)**
- Navigate to dashboard
- Select "saved" in `select[name="status"]`
- Click Filter, wait for networkidle
- Assert only 1 row visible (the saved job)
- Verify that row contains "saved" status badge text

Each test method takes `self, page, live_server, seeded_db` as parameters.
  </action>
  <verify>
Run `uv run pytest tests/e2e/test_dashboard.py -m e2e -p no:socket --headed -v` and verify all 4 tests pass. If running headless, remove `--headed`. Minimum command: `uv run pytest tests/e2e/test_dashboard.py -m e2e -p no:socket -v`.
  </verify>
  <done>4 E2E tests pass: dashboard loads with 10 jobs, filtering by platform shows 4 indeed jobs, filtering by score 4+ shows correct count, filtering by status "saved" shows 1 job.</done>
</task>

<task type="auto">
  <name>Task 3: Write status change persistence E2E test (E2E-03)</name>
  <files>tests/e2e/test_status.py</files>
  <action>
Create `tests/e2e/test_status.py` with class `TestStatusUpdateE2E` decorated with `@pytest.mark.e2e` and `@pytest.mark.enable_socket`.

Add `from __future__ import annotations` and `from playwright.sync_api import expect`.

**Test 1: `test_status_change_via_detail_page_persists`**
- Navigate to dashboard: `page.goto(f"{live_server}/")`
- Click the first job row to go to detail page: `page.locator("#job-table-body tr.job-row").first.click()`
- Wait for detail page: `page.wait_for_load_state("networkidle")`
- Verify we're on the detail page by checking the job title heading is visible: `expect(page.locator("h1")).to_be_visible()`
- Store the current status text from `#status-display`
- Change status to "applied" using the select: `page.select_option('select[name="status"]', "applied")`
- Click the "Update" button: `page.click('button:has-text("Update")')`
- Wait for the htmx response that targets `#status-display`. Use `page.wait_for_response(lambda r: "/status" in r.url)` to wait for the POST to complete
- Verify the status display updated: `expect(page.locator("#status-display")).to_contain_text("Applied")`
- Reload the page: `page.reload(); page.wait_for_load_state("networkidle")`
- Verify persistence: `expect(page.locator("#status-display")).to_contain_text("Applied")`

**Test 2: `test_status_change_reflected_on_dashboard`**
- Navigate to dashboard, click first job, change status to "applied" (same flow as above)
- Navigate back to dashboard: `page.goto(f"{live_server}/")`
- Wait for load
- Filter by status "applied": select "applied" in status dropdown, click Filter
- Verify at least 1 row is visible with "applied" badge

Each test method takes `self, page, live_server, seeded_db` as parameters.
  </action>
  <verify>
Run `uv run pytest tests/e2e/test_status.py -m e2e -p no:socket -v` and verify both tests pass.
  </verify>
  <done>2 E2E tests pass: status change via detail page persists after reload, and the change is reflected when filtering on the dashboard.</done>
</task>

</tasks>

<verification>
Run the complete E2E test suite to verify all tests pass:
```bash
uv run pytest tests/e2e/ -m e2e -p no:socket -v
```
Expected: 6 tests pass (4 dashboard + 2 status).

Verify no interference with existing test suite:
```bash
uv run pytest --co -q | tail -5
```
Expected: E2E tests are NOT collected by default (excluded by `-m "not e2e"` in addopts).
</verification>

<success_criteria>
- pytest-playwright is installed as a dev dependency
- tests/e2e/ package exists with conftest.py, test_dashboard.py, test_status.py
- `uv run pytest tests/e2e/ -m e2e -p no:socket -v` passes all 6 tests
- `uv run pytest` (default) does NOT collect E2E tests
- Dashboard loads with seeded job data visible in browser
- Filtering by platform/score/status returns correct subsets
- Status changes persist across page reloads
</success_criteria>

<output>
After completion, create `.planning/phases/15-e2e-tests/15-01-SUMMARY.md`
</output>
