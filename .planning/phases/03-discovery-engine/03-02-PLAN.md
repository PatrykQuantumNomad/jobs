---
phase: 03-discovery-engine
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - webapp/db.py
  - orchestrator.py
autonomous: true

must_haves:
  truths:
    - "SQLite schema migrates cleanly from version 0 to 2 with all new columns (first_seen_at, last_seen_at, viewed_at, score_breakdown, company_aliases, salary_display, salary_currency)"
    - "upsert_job preserves first_seen_at on conflict while updating last_seen_at"
    - "Orchestrator phase_3_score uses salary.parse_salary, dedup.fuzzy_deduplicate, and scorer.score_job_with_breakdown instead of the old inline logic"
    - "Stale job removal only deletes from platforms that were searched this run, not all platforms"
    - "Existing jobs get re-scored with breakdowns during migration backfill"
  artifacts:
    - path: "webapp/db.py"
      provides: "Versioned schema migration, delta tracking functions, updated upsert"
      contains: "PRAGMA user_version"
    - path: "orchestrator.py"
      provides: "Wired pipeline using new salary/dedup/scorer modules with delta tracking"
      contains: "fuzzy_deduplicate"
  key_links:
    - from: "orchestrator.py"
      to: "salary.py"
      via: "import parse_salary, parse_salary_ints"
      pattern: "from salary import"
    - from: "orchestrator.py"
      to: "dedup.py"
      via: "import fuzzy_deduplicate"
      pattern: "from dedup import"
    - from: "orchestrator.py"
      to: "scorer.py"
      via: "score_batch_with_breakdown replaces score_batch"
      pattern: "score_batch_with_breakdown\\|score_job_with_breakdown"
    - from: "orchestrator.py"
      to: "webapp/db.py"
      via: "upsert_job with new columns, remove_stale_jobs"
      pattern: "remove_stale_jobs"
    - from: "webapp/db.py"
      to: "webapp/db.py"
      via: "migrate_db called on init_db, PRAGMA user_version tracks state"
      pattern: "migrate_db"
---

<objective>
Wire the new salary/dedup/scorer modules into the database layer and orchestrator pipeline, with versioned schema migration and delta tracking.

Purpose: This connects the new processing modules (from Plan 01) to the persistence layer and pipeline, enabling salary normalization, fuzzy dedup, score breakdowns, and new-job detection to actually function end-to-end.
Output: Updated `webapp/db.py` with migration and new functions, updated `orchestrator.py` with wired pipeline.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-discovery-engine/03-RESEARCH.md
@.planning/phases/03-discovery-engine/03-CONTEXT.md
@.planning/phases/03-discovery-engine/03-01-SUMMARY.md

@webapp/db.py
@orchestrator.py
@models.py
@salary.py
@dedup.py
@scorer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add versioned schema migration and new DB functions to webapp/db.py</name>
  <files>webapp/db.py</files>
  <action>
Modify `webapp/db.py` to add versioned schema migration and new functions for delta tracking:

**Schema migration system:**
1. Add `SCHEMA_VERSION = 2` constant
2. Add `MIGRATIONS` dict mapping version numbers to lists of SQL statements:
   - Version 1: empty (original schema, already applied via CREATE TABLE IF NOT EXISTS)
   - Version 2 (Phase 3): Seven ALTER TABLE statements (one per new column):
     - `ALTER TABLE jobs ADD COLUMN first_seen_at TEXT`
     - `ALTER TABLE jobs ADD COLUMN last_seen_at TEXT`
     - `ALTER TABLE jobs ADD COLUMN viewed_at TEXT`
     - `ALTER TABLE jobs ADD COLUMN score_breakdown TEXT` (stores JSON)
     - `ALTER TABLE jobs ADD COLUMN company_aliases TEXT` (stores JSON array)
     - `ALTER TABLE jobs ADD COLUMN salary_display TEXT` (formatted string)
     - `ALTER TABLE jobs ADD COLUMN salary_currency TEXT DEFAULT 'USD'`
     - Backfill: `UPDATE jobs SET first_seen_at = created_at WHERE first_seen_at IS NULL`
     - Backfill: `UPDATE jobs SET last_seen_at = updated_at WHERE last_seen_at IS NULL`
3. Add `migrate_db(conn)` function that reads `PRAGMA user_version`, runs pending migrations, sets new version. Each ALTER TABLE wrapped in try/except for idempotency (catch "duplicate column name" OperationalError, re-raise others).
4. Call `migrate_db(conn)` inside `init_db()` after `CREATE TABLE IF NOT EXISTS`.

**Updated upsert_job:**
- Add new columns to INSERT statement: `first_seen_at, last_seen_at, viewed_at, score_breakdown, company_aliases, salary_display, salary_currency`
- On INSERT: set `first_seen_at = now`, `last_seen_at = now`
- ON CONFLICT: preserve `first_seen_at` (do NOT update), update `last_seen_at = excluded.last_seen_at`, update `score_breakdown`, `company_aliases`, `salary_display`, `salary_currency` from excluded. Do NOT reset `viewed_at` on conflict (preserve user's viewed state).
- `score_breakdown` and `company_aliases` should be JSON-serialized if they're dicts/lists in the input.

**New functions:**
1. `mark_viewed(dedup_key: str) -> None`: Set `viewed_at = now` WHERE `dedup_key = ?` AND `viewed_at IS NULL`.
2. `remove_stale_jobs(searched_platforms: list[str], run_timestamp: str) -> int`: DELETE FROM jobs WHERE platform IN (searched_platforms) AND (last_seen_at IS NULL OR last_seen_at < run_timestamp). Return rowcount. CRITICAL: Only delete from platforms that were actually searched this run, per research pitfall #3.
3. `backfill_score_breakdowns(scorer_fn) -> int`: Load all jobs with score IS NOT NULL AND score_breakdown IS NULL. For each, call `scorer_fn(Job(**row))` to get breakdown, update score_breakdown column. Return count. This is for the one-time migration per research recommendation.

**Important anti-pattern to avoid:** Do NOT delete stale jobs during search. The `remove_stale_jobs` function is called AFTER the full pipeline completes.
  </action>
  <verify>
Run: `python -c "
import os, tempfile, json
# Use a temp DB to test migration
os.environ['JOBFLOW_TEST_DB'] = '1'
import webapp.db as db

# Test migration runs without error on fresh DB
db.init_db()

# Test upsert with new fields
db.upsert_job({
    'id': 'test1', 'platform': 'indeed', 'title': 'Staff Eng',
    'company': 'Google', 'url': 'http://x', 'location': 'Remote',
    'salary_display': '\$200K-\$250K USD/yr', 'salary_currency': 'USD',
    'score_breakdown': json.dumps({'title': 2, 'tech': 2, 'remote': 1, 'salary': 1, 'total': 5}),
    'company_aliases': json.dumps(['Google LLC']),
    'score': 5, 'status': 'scored',
})
job = db.get_job('google::staff eng')
assert job is not None
assert job['salary_display'] == '\$200K-\$250K USD/yr'
assert job['first_seen_at'] is not None
assert job['last_seen_at'] is not None
print('upsert OK')

# Test mark_viewed
db.mark_viewed('google::staff eng')
job = db.get_job('google::staff eng')
assert job['viewed_at'] is not None
print('mark_viewed OK')

# Test remove_stale_jobs
count = db.remove_stale_jobs(['indeed'], '9999-01-01T00:00:00')
assert count >= 1
print(f'remove_stale_jobs removed {count}')
print('db OK')
"
`

Note: If the test uses the real DB path, the tester should temporarily point to a test DB or clean up. The verification is conceptual -- adapt path handling if needed.
  </verify>
  <done>Schema migration adds all 7 new columns idempotently via PRAGMA user_version. upsert_job handles new fields with first_seen_at preserved on conflict. mark_viewed and remove_stale_jobs functions work correctly. Stale removal is platform-scoped.</done>
</task>

<task type="auto">
  <name>Task 2: Wire new modules into orchestrator.py phase_3_score and add delta tracking</name>
  <files>orchestrator.py</files>
  <action>
Refactor `orchestrator.py` to use the new salary, dedup, and scorer modules, and add delta tracking:

**Imports:**
- Add: `from salary import parse_salary, parse_salary_ints, NormalizedSalary`
- Add: `from dedup import fuzzy_deduplicate`
- Add: `from webapp import db as webdb` (for delta persistence)
- Keep existing imports

**Modify `__init__`:**
- Store `self.searched_platforms: list[str] = []` to track which platforms were actually searched (for stale removal)
- Store `self.run_timestamp: str = ""` to mark current run

**Modify `run()`:**
- Set `self.run_timestamp = datetime.now().isoformat()` at start
- Store `self.searched_platforms = [p for p in platforms if p not in self._failed_logins]` after phase_1_login
- After `phase_3_score()`, call delta cleanup: `stale_count = webdb.remove_stale_jobs(self.searched_platforms, self.run_timestamp)` and print count
- Add one-time backfill call: after scoring, if any jobs lack breakdowns, run `webdb.backfill_score_breakdowns(...)` (only needed on first run after migration)

**Refactor `phase_3_score()`:**
Replace the existing inline scoring flow with:

1. Load raw results (existing `_load_raw_results()`)
2. **Salary normalization** (NEW): For each job, parse salary using `parse_salary(job.salary)` for string salaries or `parse_salary_ints(job.salary_min, job.salary_max)` for RemoteOK integers. Update `job.salary_min`, `job.salary_max`, `job.salary_display`, `job.salary_currency`.
3. **Fuzzy deduplicate** (REPLACE `_deduplicate`): Call `fuzzy_deduplicate(all_jobs)` instead of `self._deduplicate(all_jobs)`. Log merge count.
4. **Score with breakdowns** (REPLACE `score_batch`): Call `self.scorer.score_batch_with_breakdown(unique)` to get list of `(Job, ScoreBreakdown)` tuples.
5. **Persist to DB** (NEW): For each scored job + breakdown, build a dict with all fields including `score_breakdown=json.dumps(breakdown.to_dict())`, `company_aliases=json.dumps(job.company_aliases)`, `salary_display=job.salary_display`, `salary_currency=job.salary_currency`, `last_seen_at=self.run_timestamp`. Call `webdb.upsert_job(job_dict)`. Set `first_seen_at` only on insert (handled by db.py ON CONFLICT logic).
6. Filter score 3+ and save JSON/descriptions/tracker as before.

**Keep `_deduplicate` method** but mark it as deprecated (or remove it -- the new `fuzzy_deduplicate` replaces it entirely).

**Update `_save_descriptions`:** Use `job.salary_display` instead of the inline salary formatting. If `salary_display` is empty, don't show salary line at all (per user decision: "don't show a salary field at all").

**Update `_write_tracker`:** Use `job.salary_display` instead of inline formatting.

**Update `phase_4_apply` display:** Use `job.salary_display` instead of inline salary formatting.

**Update `_print_summary`:** Add count of newly discovered jobs (those with `first_seen_at == self.run_timestamp`, or approximate by counting upserts that were inserts vs updates).
  </action>
  <verify>
Run: `python -c "
from orchestrator import Orchestrator
# Verify imports work
from salary import parse_salary
from dedup import fuzzy_deduplicate
from scorer import ScoreBreakdown
print('All orchestrator imports OK')

# Verify Orchestrator can be instantiated
o = Orchestrator()
assert hasattr(o, 'searched_platforms')
assert hasattr(o, 'run_timestamp')
print('Orchestrator init OK')
"
`

Run: `python -c "
# Verify the scoring flow works end-to-end with mock data
from models import Job
from salary import parse_salary
from dedup import fuzzy_deduplicate
from scorer import JobScorer

jobs = [
    Job(platform='indeed', title='Staff Engineer', company='Google', url='http://a', salary='\$200,000 - \$250,000', location='Remote', description='kubernetes python'),
    Job(platform='dice', title='Staff Engineer', company='Google LLC', url='http://b', salary='USD 200,000.00 - 250,000.00 per year', location='Remote', description='kubernetes python terraform'),
]

# Salary normalize
for j in jobs:
    sal = parse_salary(j.salary)
    j.salary_min = sal.min_annual
    j.salary_max = sal.max_annual
    j.salary_display = sal.display
    j.salary_currency = sal.currency

# Dedup
unique = fuzzy_deduplicate(jobs)
assert len(unique) == 1, f'Expected 1, got {len(unique)}'
assert len(unique[0].company_aliases) >= 1

# Score
scorer = JobScorer()
results = scorer.score_batch_with_breakdown(unique)
assert len(results) == 1
job, breakdown = results[0]
assert job.score is not None
assert breakdown.total == job.score
print(f'End-to-end: {job.company} score={job.score} breakdown={breakdown.display_inline()}')
print(f'Aliases: {job.company_aliases}')
print(f'Salary: {job.salary_display}')
print('Pipeline flow OK')
"
`
  </verify>
  <done>orchestrator.py uses salary.parse_salary for all salary normalization, dedup.fuzzy_deduplicate for dedup (replacing inline _deduplicate), scorer.score_batch_with_breakdown for scoring, and webdb.upsert_job/remove_stale_jobs for delta tracking. Salary display in descriptions/tracker/apply uses job.salary_display. Stale removal scoped to searched platforms only.</done>
</task>

</tasks>

<verification>
1. `python -c "from webapp.db import init_db, get_conn; init_db(); c = get_conn(); v = c.execute('PRAGMA user_version').fetchone()[0]; assert v >= 2, f'version {v}'; print(f'Schema version: {v}')"` -- migration applied
2. Schema has all 7 new columns: `python -c "from webapp.db import get_conn; c = get_conn(); cols = [r[1] for r in c.execute('PRAGMA table_info(jobs)').fetchall()]; assert 'first_seen_at' in cols; assert 'score_breakdown' in cols; assert 'salary_display' in cols; print('All columns present')"`
3. Orchestrator instantiates without error and has new attributes
4. Full pipeline flow (salary -> dedup -> score -> persist) works with mock data
5. Stale removal only targets searched platforms
</verification>

<success_criteria>
- Schema migrates from v0 to v2 with all new columns, idempotently
- upsert_job persists salary_display, salary_currency, score_breakdown (JSON), company_aliases (JSON), first_seen_at, last_seen_at
- first_seen_at preserved on conflict (not overwritten on re-upsert)
- remove_stale_jobs scoped to searched platforms only
- orchestrator.phase_3_score uses new modules instead of inline logic
- Pipeline flow: raw jobs -> salary normalize -> fuzzy dedup -> score with breakdown -> persist to DB -> remove stale
</success_criteria>

<output>
After completion, create `.planning/phases/03-discovery-engine/03-02-SUMMARY.md`
</output>
